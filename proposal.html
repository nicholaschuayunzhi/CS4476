<!DOCTYPE html>
<html>
<head>
    
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Proposal</title>
    <link rel="stylesheet" href="markbind\css\bootstrap.min.css">
    <link rel="stylesheet" href="markbind\css\bootstrap-vue.min.css">
    <link rel="stylesheet" href="markbind\css\font-awesome.min.css" >
    <link rel="stylesheet" href="markbind\css\bootstrap-glyphicons.min.css" >
    <link rel="stylesheet" href="markbind\css\github.min.css">
    <link rel="stylesheet" href="markbind\css\markbind.css">
    
    
    
</head>
<body>
<div id="app">
    <div id="content-wrapper">
  <div>
    <navbar placement="top" type="inverse">
      <a slot="brand" href="/CS4476" title="Home" class="navbar-brand">Cave Street View</a>
      <li><a href="/CS4476/proposal.html" class="nav-link">Proposal</a></li>
      <li><a href="/CS4476/midterm.html" class="nav-link">Midterm Update</a></li>
      <li><a href="https://github.com/anajib6/Cave-View" target="_blank" class="nav-link">Code <span class="fab fa-github"></span></a></li>
    </navbar>
    <br></div>
  <h1 id="approach">Approach<a class="fa fa-anchor" href="#approach"></a></h1>
  <h2 id="general-process-pipeline">General Process Pipeline<a class="fa fa-anchor" href="#general-process-pipeline"></a></h2>
  <ol>
    <li>Caver takes multiple photos</li>
    <li>Caver uses software to stitch images together</li>
    <li>Software automatically stitches images into one giant image
      <ul>
        <li>The image should have sufficient information to wrap around in the x, y and z directions
          <ul>
            <li>If it does not have enough information then the non-captured areas will just be blacked-out</li>
          </ul>
        </li>
        <li>Blending and image correction is done by the software to account for issues with lighting in the cave</li>
      </ul>
    </li>
    <li>Software then maps 360 image into 3d space for viewing
      <ul>
        <li>Caver can view this image preferably in a GUI</li>
      </ul>
    </li>
  </ol>
  <h1 id="experiment-and-results">Experiment and Results<a class="fa fa-anchor" href="#experiment-and-results"></a></h1>
  <h2 id="dataset">Dataset:<a class="fa fa-anchor" href="#dataset"></a></h2>
  <p>We will be generating our own dataset for this project, as there does not openly exist a dataset of 360 degree cave images. In order to build this dataset, we will collect pictures from many different locations in some caves.</p>
  <ul>
    <li>For each location, the cameraman should not move the origin of the camera (only rotate)</li>
    <li>Number of pictures taken should roughly cover the full 360 area of the cave</li>
  </ul>
  <h2 id="exploited-code">Exploited Code:<a class="fa fa-anchor" href="#exploited-code"></a></h2>
  <ul>
    <li><a href="https://opencv.org/">OpenCV</a>
      <ul>
        <li>implementation of <a href="https://www.pyimagesearch.com/2016/01/11/opencv-panorama-stitching/">base stitcher</a></li>
      </ul>
    </li>
  </ul>
  <h2 id="implemented-code">Implemented Code:<a class="fa fa-anchor" href="#implemented-code"></a></h2>
  <ul>
    <li>Cylindrical Warp</li>
    <li>Alpha Blending</li>
    <li>Multi-image 3D Mapping</li>
    <li>Lighting Filter</li>
    <li>GUI for input and display</li>
  </ul>
  <h2 id="success-criteria">Success Criteria:<a class="fa fa-anchor" href="#success-criteria"></a></h2>
  <p>The project will be successful if it generates 3D images from cave interior pictures, with the images having lighting artifacts removed, and thus generating a clear 360 image.</p>
  <h2 id="experiments">Experiments<a class="fa fa-anchor" href="#experiments"></a></h2>
  <p><strong>Experiment 1:</strong> Stitch 2 Images together without manually setting correspondecnes</p>
  <ol>
    <li>Manually select images to stitch together</li>
    <li>Use SIFT to identify correspondences (OpenCV)</li>
    <li>Image stitching using homographies</li>
    <li>Blend images together with different blending techniques</li>
  </ol>
  <p><strong>Experiment 2:</strong> Stitch all images from cave location into large image</p>
  <ol>
    <li>SIFT to identify correspondences</li>
    <li>Use k-nn model to find matching features for each feature in the images</li>
    <li>Use RANSAC to find homographies between images with these features</li>
    <li>Generate the full cave image</li>
  </ol>
  <p><strong>Experiment 3:</strong> Conversion of stitched images into 3D space</p>
  <ol>
    <li>Mapping of images to spherical/cylindrical coordinates</li>
    <li>Apply final stitched image to sphere/cylinder via texture mapping</li>
  </ol>
  <p><strong>Experiment 4:</strong> Image correction due to cave lighting issues</p>
  <ol>
    <li>We will work with both single images and the 3D stitched images to look at what works for fixing the lighting</li>
    <li>Take shots with varying brightness to do flash reflection correction</li>
    <li>Remove artifacts from noise or insufficient image data
      <ul>
        <li>Image inpainting</li>
        <li>Texture creation with Markov Models</li>
      </ul>
    </li>
  </ol>
  <h2 id="experiment-results">Experiment Results<a class="fa fa-anchor" href="#experiment-results"></a></h2>
  <p>These experiments should allow us to build up to the final result of the project, an application that can take in many images from a cave, stitch them together into a 3D space, and apply some filters to fix lighting issues that arise when taking pictures
    in caves with non-professional photography equipment.</p>
  <p>The first 3 experiments are image manipulation in physical space. As caves tend to have many unique and unusual features we should be able to form decent mappings of features, but <strong>Experiment 1</strong> will focus on making sure this is the case.
    Another focus of <strong>Experiment 1</strong> would be to test out various blending techniques and compare them with one another.</p>
  <p><strong>Experiment 2</strong> will follow this trend and check how well the feature mapping works over the entire cave space.</p>
  <p><strong>Experiment 3</strong> will determine both whether or not cylindrical or spherical projections work better for this task, as well as making sure that the feature mapping still works when not applied to a flat plane.</p>
  <p>The major unknown with these experiments is whether or not the feature selection will work well, though we are fairly confident that it will.</p>
  <p><strong>Experiment 4</strong> will work with finding the best filters and processing that needs to be done to the images to remove lighting issues and create clearer images.</p>
  <hr>
</div>
<div id="flex-div"></div>
<footer>
  <div class="text-center">
    This is a dynamic height footer that supports variables <span class="glyphicon glyphicon-tags" aria-hidden="true"></span> and markdown <span>ðŸ˜„</span>!
  </div>

  <div class="text-center">
    <small>[Generated by <a href="https://markbind.github.io/markbind/">MarkBind 1.13.0</a> on Wed, 31 Oct 2018 13:45:45 GMT]</small></div>
</footer>
</div>
</body>
<script src="markbind\js\vue.min.js"></script>
<script src="markbind\js\vue-strap.min.js"></script>
<script src="markbind\js\polyfill.min.js"></script>
<script src="markbind\js\bootstrap-vue.min.js"></script>
<script>
    const baseUrl = '/CS4476'
</script>
<script src="markbind\js\setup.js"></script>
</html>
